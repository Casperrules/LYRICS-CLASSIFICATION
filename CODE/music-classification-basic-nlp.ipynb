{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "93300c4a415f38386c3bba9ee8e0aa14eebdc844"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stopwords_eng.txt', 'valid_lyrics_rem_200.csv', 'train_lyrics_rem_1000.csv', 'valid_lyrics_200.csv', 'train_lyrics_1000.csv', 'train_1000.csv', 'label_encoder.p.txt', 'stopwords.p']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "420a5d876956c6f183319d4f1abbe5893718404b"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-42447774e7a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/train_lyrics_1000.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvalidate\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/valid_lyrics_200.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df1' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "train = pd.read_csv('../input/train_lyrics_1000.csv', usecols=range(7))\n",
    "validate =pd.read_csv('../input/valid_lyrics_200.csv')\n",
    "print(df1.head())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "451175f3ab1c02e841932ef20f0d3e627e940df9"
   },
   "source": [
    "**1 FEATURE EXTRACTION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0716fa7e310d6b1790dd4c37ab93f67c31c7e1f2"
   },
   "source": [
    "**1.1 Number of Words**\n",
    "One of the most basic features we can extract is the number of words in each lyrics. The basic intuition behind this is that generally, the negative sentiments contain a lesser amount of words than the positive ones.[](http://)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "10c135ca235a0438e7650cf7763da11b73bf9ba9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyrics</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Verse One:\\n\\nAlright I might\\nHave had a litt...</td>\n",
       "      <td>392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adam Ant/Marco Pirroni\\nEvery girl is a someth...</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I've just erased it's been a while, I've got a...</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Little darling \\nWhere you've been so long \\nI...</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lead Vocal by Greg\\n\\nWell, these late night c...</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              lyrics  word_count\n",
       "0  Verse One:\\n\\nAlright I might\\nHave had a litt...         392\n",
       "1  Adam Ant/Marco Pirroni\\nEvery girl is a someth...         114\n",
       "2  I've just erased it's been a while, I've got a...         110\n",
       "3  Little darling \\nWhere you've been so long \\nI...         183\n",
       "4  Lead Vocal by Greg\\n\\nWell, these late night c...         182"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['word_count'] = train['lyrics'].apply(lambda x: len(str(x).split(\" \")))\n",
    "train[['lyrics','word_count']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "86206c7835d4d36bd462c40df67c2ee33c5f7a52"
   },
   "source": [
    "**1.2 Number of characters**\n",
    "This feature is also based on the previous feature intuition. Here, we calculate the number of characters in each lyrics. This is done by calculating the length of the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "4657edbb3ec76436622cef6f15e5c234f55d4c2f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyrics</th>\n",
       "      <th>char_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Verse One:\\n\\nAlright I might\\nHave had a litt...</td>\n",
       "      <td>2181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adam Ant/Marco Pirroni\\nEvery girl is a someth...</td>\n",
       "      <td>713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I've just erased it's been a while, I've got a...</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Little darling \\nWhere you've been so long \\nI...</td>\n",
       "      <td>977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lead Vocal by Greg\\n\\nWell, these late night c...</td>\n",
       "      <td>1130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              lyrics  char_count\n",
       "0  Verse One:\\n\\nAlright I might\\nHave had a litt...        2181\n",
       "1  Adam Ant/Marco Pirroni\\nEvery girl is a someth...         713\n",
       "2  I've just erased it's been a while, I've got a...         600\n",
       "3  Little darling \\nWhere you've been so long \\nI...         977\n",
       "4  Lead Vocal by Greg\\n\\nWell, these late night c...        1130"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['char_count'] = train['lyrics'].str.len() ## this also includes spaces\n",
    "train[['lyrics','char_count']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "419437f2a2facd4811abe527f56abb569fd6d3fb"
   },
   "source": [
    "**1.3 Average Word Length**\n",
    "We will also extract another feature which will calculate the average word length of each lyrics. This can also potentially help us in improving our model.\n",
    "\n",
    "Here, we simply take the sum of the length of all the words and divide it by the total length of the tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "09ed33dfce3d62e7475c49d2c4452afdb8b596c4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyrics</th>\n",
       "      <th>avg_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Verse One:\\n\\nAlright I might\\nHave had a litt...</td>\n",
       "      <td>3.742358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adam Ant/Marco Pirroni\\nEvery girl is a someth...</td>\n",
       "      <td>4.211679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I've just erased it's been a while, I've got a...</td>\n",
       "      <td>3.869919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Little darling \\nWhere you've been so long \\nI...</td>\n",
       "      <td>4.147541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lead Vocal by Greg\\n\\nWell, these late night c...</td>\n",
       "      <td>4.136986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              lyrics  avg_word\n",
       "0  Verse One:\\n\\nAlright I might\\nHave had a litt...  3.742358\n",
       "1  Adam Ant/Marco Pirroni\\nEvery girl is a someth...  4.211679\n",
       "2  I've just erased it's been a while, I've got a...  3.869919\n",
       "3  Little darling \\nWhere you've been so long \\nI...  4.147541\n",
       "4  Lead Vocal by Greg\\n\\nWell, these late night c...  4.136986"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def avg_word(sentence):\n",
    "  words = sentence.split()\n",
    "  return (sum(len(word) for word in words)/len(words))\n",
    "\n",
    "train['avg_word'] = train['lyrics'].apply(lambda x: avg_word(x))\n",
    "train[['lyrics','avg_word']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b3c22ec5f7c300fe4ed537d89995c27fa8ae2ebe"
   },
   "source": [
    "**1.4 Number of stopwords**\n",
    "Generally, while solving an NLP problem, the first thing we do is to remove the stopwords. But sometimes calculating the number of stopwords can also give us some extra information which we might have been losing before.\n",
    "\n",
    "Here, we have imported stopwords from NLTK, which is a basic NLP library in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "fa4ca7f98006d3d0cd3f585ae9de3982fed37d38"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyrics</th>\n",
       "      <th>stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Verse One:\\n\\nAlright I might\\nHave had a litt...</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adam Ant/Marco Pirroni\\nEvery girl is a someth...</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I've just erased it's been a while, I've got a...</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Little darling \\nWhere you've been so long \\nI...</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lead Vocal by Greg\\n\\nWell, these late night c...</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              lyrics  stopwords\n",
       "0  Verse One:\\n\\nAlright I might\\nHave had a litt...        163\n",
       "1  Adam Ant/Marco Pirroni\\nEvery girl is a someth...         48\n",
       "2  I've just erased it's been a while, I've got a...         47\n",
       "3  Little darling \\nWhere you've been so long \\nI...         66\n",
       "4  Lead Vocal by Greg\\n\\nWell, these late night c...         78"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "train['stopwords'] = train['lyrics'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "train[['lyrics','stopwords']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5697bf597aa32d485d3368c4f34c012a3d6cbdb6"
   },
   "source": [
    "**1.5 Number of numerics**\n",
    "Just like we calculated the number of words, we can also calculate the number of numerics which are present in the lyrics. It does not have a lot of use in our example, but this is still a useful feature that should be run while doing similar exercises. For example, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "1dbb7b2601766b8290bcfe9aa16a9a52de3e71a5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyrics</th>\n",
       "      <th>numerics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Verse One:\\n\\nAlright I might\\nHave had a litt...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adam Ant/Marco Pirroni\\nEvery girl is a someth...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I've just erased it's been a while, I've got a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Little darling \\nWhere you've been so long \\nI...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lead Vocal by Greg\\n\\nWell, these late night c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              lyrics  numerics\n",
       "0  Verse One:\\n\\nAlright I might\\nHave had a litt...         0\n",
       "1  Adam Ant/Marco Pirroni\\nEvery girl is a someth...         0\n",
       "2  I've just erased it's been a while, I've got a...         0\n",
       "3  Little darling \\nWhere you've been so long \\nI...         0\n",
       "4  Lead Vocal by Greg\\n\\nWell, these late night c...         1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['numerics'] = train['lyrics'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "train[['lyrics','numerics']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f4ec5997a65d55cee39ad926d2931cff5cba33b0"
   },
   "source": [
    "**2. Basic Pre-processing**\n",
    "So far, we have learned how to extract basic features from text data. Before diving into text and feature extraction, our first step should be cleaning the data in order to obtain better features. We will achieve this by doing some of the basic pre-processing steps on our training data.\n",
    "\n",
    "So, let’s get into it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b963741ff65ef25110d2923e16aceb1413968c2f"
   },
   "source": [
    "**2.1 Lower case**\n",
    "The first pre-processing step which we will do is transform our lyrics into lower case. This avoids having multiple copies of the same words. For example, while calculating the word count, ‘Analytics’ and ‘analytics’ will be taken as different words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "480097e39b3a94487e9da5825a4c7ef6ab86173e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    verse one: alright i might have had a little g...\n",
       "1    adam ant/marco pirroni every girl is a somethi...\n",
       "2    i've just erased it's been a while, i've got a...\n",
       "3    little darling where you've been so long i've ...\n",
       "4    lead vocal by greg well, these late night conv...\n",
       "Name: lyrics, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['lyrics'] = train['lyrics'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "train['lyrics'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1eefca9c9bb0ca954f812600a4af444974638f67"
   },
   "source": [
    "**2.2 Removing Punctuation**\n",
    "The next step is to remove punctuation, as it doesn’t add any extra information while treating text data. Therefore removing all instances of it will help us reduce the size of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "68fc920bec07b06a7e683d7f8b56327aae8bd7a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    verse one alright i might have had a little gl...\n",
       "1    adam antmarco pirroni every girl is a somethin...\n",
       "2    ive just erased its been a while ive got a wor...\n",
       "3    little darling where youve been so long ive be...\n",
       "4    lead vocal by greg well these late night conve...\n",
       "Name: lyrics, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['lyrics'] = train['lyrics'].str.replace('[^\\w\\s]','')\n",
    "train['lyrics'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4879e2c572dd79781b5670fe96ae7f85ded061e2"
   },
   "source": [
    "**2.3 Removal of Stop Words**\n",
    "As we discussed earlier, stop words (or commonly occurring words) should be removed from the text data. For this purpose, we can either create a list of stopwords ourselves or we can use predefined libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "3a2664048bc359d5a469596fb34b72dea23c3268"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    verse one alright might little glare stared ya...\n",
       "1    adam antmarco pirroni every girl something gir...\n",
       "2    ive erased ive got world sale walk away better...\n",
       "3    little darling youve long ive thinking ya feel...\n",
       "4    lead vocal greg well late night conversations ...\n",
       "Name: lyrics, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "train['lyrics'] = train['lyrics'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "train['lyrics'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ab8f0351e0c81e5864e2541bfbde375636281964"
   },
   "source": [
    "**2.4 Rare words removal**\n",
    " we can remove the most common words as well as rare words, this time let’s remove rarely occurring words from the text. Because they’re so rare, the association between them and other words is dominated by noise. You can replace rare words with a more general form and then this will have higher counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "a1931581aa75072c10041e7054dbdc1904738057"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "brace        1\n",
       "trades       1\n",
       "noo          1\n",
       "worryin      1\n",
       "buckwild     1\n",
       "portafide    1\n",
       "plugged      1\n",
       "pretended    1\n",
       "withyeah     1\n",
       "angelic      1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = pd.Series(' '.join(train['lyrics']).split()).value_counts()[-10:]\n",
    "freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1a3d231f5780176731bb232a78c36bd5daefc4a0"
   },
   "source": [
    "**2.5 Tokenization**\n",
    "Tokenization refers to dividing the text into a sequence of words or sentences. In our example, we have used the textblob library to first transform our lyrics into a blob and then converted them into a series of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "1fa7b17a10a09869ab4234a99ed3883b6e86d7e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['adam', 'antmarco', 'pirroni', 'every', 'girl', 'something', 'girl', 'every', 'girl', 'something', 'girl', 'every', 'girl', 'something', 'girl', 'shes', 'got', 'something', 'every', 'girl', 'something', 'girl', 'every', 'girl', 'somehing', 'smile', 'every', 'girl', 'something', 'girl', 'three', 'wishes', 'know', 'mine', 'would', 'ask', 'stars', 'shine', 'wouldnt', 'ask', 'gems', 'pearls', 'id', 'ask', 'something', 'girls', 'every', 'girl', 'something', 'girl', 'whites', 'eyes', 'tip', 'curl', 'every', 'girl', 'sends', 'something', 'boy', 'something', 'old', 'world', 'wont', 'treat', 'right', 'youre', 'feeling', 'tonight', 'think', 'someone', 'relax', 'let', 'something', 'every', 'girl', 'something', 'girl', 'shes', 'got', 'shes', 'got', 'something'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "TextBlob(train['lyrics'][1]).words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0e8d28fc63086e564184d72dd7733c1ad469c1fa"
   },
   "source": [
    "**2.6 Stemming**\n",
    "Stemming refers to the removal of suffices, like “ing”, “ly”, “s”, etc. by a simple rule-based approach. For this purpose, we will use PorterStemmer from the NLTK library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "b01cbf4f131f56150e949b3a5c7a83e4db46128c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    vers one alright might littl glare stare ya ho...\n",
       "1    adam antmarco pirroni everi girl someth girl e...\n",
       "2    ive eras ive got world sale walk away better d...\n",
       "3    littl darl youv long ive think ya feel real st...\n",
       "4    lead vocal greg well late night convers leav s...\n",
       "Name: lyrics, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "st = PorterStemmer()\n",
    "train['lyrics'][:5].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4bcabb88a3d9a21d79181d3befa8029703ed1829"
   },
   "source": [
    "**2.7 Lemmatization**\n",
    "Lemmatization is a more effective option than stemming because it converts the word into its root word, rather than just stripping the suffices. It makes use of the vocabulary and does a morphological analysis to obtain the root word. Therefore, we usually prefer using lemmatization over stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "415432a145183bcdf9c36387734284f7177f3cda"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    verse one alright might little glare stared ya...\n",
       "1    adam antmarco pirroni every girl something gir...\n",
       "2    ive erased ive got world sale walk away better...\n",
       "3    little darling youve long ive thinking ya feel...\n",
       "4    lead vocal greg well late night conversation l...\n",
       "Name: lyrics, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import Word\n",
    "train['lyrics'] = train['lyrics'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "train['lyrics'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f07f77330e67bbc5a1f17edef7132f13a91a891a"
   },
   "source": [
    "**3. Advance Text Processing**\n",
    "Up to this point, we have done all the basic pre-processing steps in order to clean our data. Now, we can finally move on to extracting features using NLP techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9582acebf7838572328d3844bc5702590579a580"
   },
   "source": [
    "**3.1 N-grams**\n",
    "N-grams are the combination of multiple words used together. Ngrams with N=1 are called unigrams. Similarly, bigrams (N=2), trigrams (N=3) and so on can also be used.\n",
    "\n",
    "Unigrams do not usually contain as much information as compared to bigrams and trigrams. The basic principle behind n-grams is that they capture the language structure, like what letter or word is likely to follow the given one. The longer the n-gram (the higher the n), the more context you have to work with. Optimum length really depends on the application – if your n-grams are too short, you may fail to capture important differences. On the other hand, if they are too long, you may fail to capture the “general knowledge” and only stick to particular cases.\n",
    "\n",
    "So, let’s quickly extract bigrams from our tweets using the ngrams function of the textblob library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "aedfd8dda451e8f795be5ac904ecde408b06811f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['verse', 'one']),\n",
       " WordList(['one', 'alright']),\n",
       " WordList(['alright', 'might']),\n",
       " WordList(['might', 'little']),\n",
       " WordList(['little', 'glare']),\n",
       " WordList(['glare', 'stared']),\n",
       " WordList(['stared', 'ya']),\n",
       " WordList(['ya', 'ho']),\n",
       " WordList(['ho', 'didnt']),\n",
       " WordList(['didnt', 'know']),\n",
       " WordList(['know', 'like']),\n",
       " WordList(['like', 'stared']),\n",
       " WordList(['stared', 'right']),\n",
       " WordList(['right', 'back']),\n",
       " WordList(['back', 'nigga']),\n",
       " WordList(['nigga', 'warnin']),\n",
       " WordList(['warnin', 'comin']),\n",
       " WordList(['comin', 'react']),\n",
       " WordList(['react', 'like']),\n",
       " WordList(['like', 'mack']),\n",
       " WordList(['mack', 'act']),\n",
       " WordList(['act', 'cool']),\n",
       " WordList(['cool', 'test']),\n",
       " WordList(['test', 'cause']),\n",
       " WordList(['cause', 'aint']),\n",
       " WordList(['aint', 'jester']),\n",
       " WordList(['jester', 'suggest']),\n",
       " WordList(['suggest', 'friend']),\n",
       " WordList(['friend', 'outtie']),\n",
       " WordList(['outtie', 'cause']),\n",
       " WordList(['cause', 'dont']),\n",
       " WordList(['dont', 'want']),\n",
       " WordList(['want', 'make']),\n",
       " WordList(['make', 'pal']),\n",
       " WordList(['pal', 'get']),\n",
       " WordList(['get', 'rowdy']),\n",
       " WordList(['rowdy', 'doubt']),\n",
       " WordList(['doubt', 'friendship']),\n",
       " WordList(['friendship', 'lip']),\n",
       " WordList(['lip', 'touch']),\n",
       " WordList(['touch', 'go']),\n",
       " WordList(['go', 'crazy']),\n",
       " WordList(['crazy', 'clutch']),\n",
       " WordList(['clutch', 'sorta']),\n",
       " WordList(['sorta', 'like']),\n",
       " WordList(['like', 'schitzo']),\n",
       " WordList(['schitzo', 'forgets']),\n",
       " WordList(['forgets', 'bros']),\n",
       " WordList(['bros', 'pal']),\n",
       " WordList(['pal', 'gal']),\n",
       " WordList(['gal', 'didnt']),\n",
       " WordList(['didnt', 'mean']),\n",
       " WordList(['mean', 'fiend']),\n",
       " WordList(['fiend', 'strange']),\n",
       " WordList(['strange', 'thing']),\n",
       " WordList(['thing', 'denim']),\n",
       " WordList(['denim', 'matter']),\n",
       " WordList(['matter', 'who']),\n",
       " WordList(['who', 'em']),\n",
       " WordList(['em', 'grab']),\n",
       " WordList(['grab', 'flooze']),\n",
       " WordList(['flooze', 'im']),\n",
       " WordList(['im', 'traffic']),\n",
       " WordList(['traffic', 'dont']),\n",
       " WordList(['dont', 'laugh']),\n",
       " WordList(['laugh', 'might']),\n",
       " WordList(['might', 'girl']),\n",
       " WordList(['girl', 'im']),\n",
       " WordList(['im', 'talkin']),\n",
       " WordList(['talkin', 'didnt']),\n",
       " WordList(['didnt', 'mean']),\n",
       " WordList(['mean', 'chorus']),\n",
       " WordList(['chorus', 'repeat']),\n",
       " WordList(['repeat', '2x']),\n",
       " WordList(['2x', 'aint']),\n",
       " WordList(['aint', 'fault']),\n",
       " WordList(['fault', 'girl']),\n",
       " WordList(['girl', 'got']),\n",
       " WordList(['got', 'caught']),\n",
       " WordList(['caught', 'aint']),\n",
       " WordList(['aint', 'fault']),\n",
       " WordList(['fault', 'girl']),\n",
       " WordList(['girl', 'got']),\n",
       " WordList(['got', 'caught']),\n",
       " WordList(['caught', 'didnt']),\n",
       " WordList(['didnt', 'mean']),\n",
       " WordList(['mean', 'verse']),\n",
       " WordList(['verse', 'two']),\n",
       " WordList(['two', 'another']),\n",
       " WordList(['another', 'incident']),\n",
       " WordList(['incident', 'went']),\n",
       " WordList(['went', 'way']),\n",
       " WordList(['way', 'beyond']),\n",
       " WordList(['beyond', 'done']),\n",
       " WordList(['done', 'john']),\n",
       " WordList(['john', 'stopped']),\n",
       " WordList(['stopped', 'drawer']),\n",
       " WordList(['drawer', 'dropped']),\n",
       " WordList(['dropped', 'didnt']),\n",
       " WordList(['didnt', 'aint']),\n",
       " WordList(['aint', 'quittin']),\n",
       " WordList(['quittin', 'really']),\n",
       " WordList(['really', 'didnt']),\n",
       " WordList(['didnt', 'care']),\n",
       " WordList(['care', 'who']),\n",
       " WordList(['who', 'girl']),\n",
       " WordList(['girl', 'hittin']),\n",
       " WordList(['hittin', 'admit']),\n",
       " WordList(['admit', 'skinz']),\n",
       " WordList(['skinz', 'aint']),\n",
       " WordList(['aint', 'reason']),\n",
       " WordList(['reason', 'lose']),\n",
       " WordList(['lose', 'friend']),\n",
       " WordList(['friend', 'didnt']),\n",
       " WordList(['didnt', 'know']),\n",
       " WordList(['know', 'sorry']),\n",
       " WordList(['sorry', 'accept']),\n",
       " WordList(['accept', 'apology']),\n",
       " WordList(['apology', 'live']),\n",
       " WordList(['live', 'goin']),\n",
       " WordList(['goin', 'hold']),\n",
       " WordList(['hold', 'grudge']),\n",
       " WordList(['grudge', 'well']),\n",
       " WordList(['well', 'oh']),\n",
       " WordList(['oh', 'fudge']),\n",
       " WordList(['fudge', 'new']),\n",
       " WordList(['new', 'year']),\n",
       " WordList(['year', 'fear']),\n",
       " WordList(['fear', 'ya']),\n",
       " WordList(['ya', 'action']),\n",
       " WordList(['action', 'ya']),\n",
       " WordList(['ya', 'actin']),\n",
       " WordList(['actin', 'shy']),\n",
       " WordList(['shy', 'mean']),\n",
       " WordList(['mean', 'ya']),\n",
       " WordList(['ya', 'need']),\n",
       " WordList(['need', 'smackin']),\n",
       " WordList(['smackin', 'hoe']),\n",
       " WordList(['hoe', 'instead']),\n",
       " WordList(['instead', 'tryin']),\n",
       " WordList(['tryin', 'front']),\n",
       " WordList(['front', 'ya']),\n",
       " WordList(['ya', 'know']),\n",
       " WordList(['know', 'thats']),\n",
       " WordList(['thats', 'exactly']),\n",
       " WordList(['exactly', 'im']),\n",
       " WordList(['im', 'goin']),\n",
       " WordList(['goin', 'ya']),\n",
       " WordList(['ya', 'know']),\n",
       " WordList(['know', 'run']),\n",
       " WordList(['run', 'game']),\n",
       " WordList(['game', 'g']),\n",
       " WordList(['g', 'ya']),\n",
       " WordList(['ya', 'know']),\n",
       " WordList(['know', 'cause']),\n",
       " WordList(['cause', 'entertainin']),\n",
       " WordList(['entertainin', 'meant']),\n",
       " WordList(['meant', 'blame']),\n",
       " WordList(['blame', 'woman']),\n",
       " WordList(['woman', 'makin']),\n",
       " WordList(['makin', 'ak']),\n",
       " WordList(['ak', 'seem']),\n",
       " WordList(['seem', 'call']),\n",
       " WordList(['call', 'screen']),\n",
       " WordList(['screen', 'didnt']),\n",
       " WordList(['didnt', 'mean']),\n",
       " WordList(['mean', 'chorus']),\n",
       " WordList(['chorus', 'verse']),\n",
       " WordList(['verse', 'three']),\n",
       " WordList(['three', 'fool']),\n",
       " WordList(['fool', 'schememin']),\n",
       " WordList(['schememin', 'dreamin']),\n",
       " WordList(['dreamin', 'actin']),\n",
       " WordList(['actin', 'like']),\n",
       " WordList(['like', 'demon']),\n",
       " WordList(['demon', 'nigga']),\n",
       " WordList(['nigga', 'dont']),\n",
       " WordList(['dont', 'give']),\n",
       " WordList(['give', 'fuck']),\n",
       " WordList(['fuck', 'beat']),\n",
       " WordList(['beat', 'shit']),\n",
       " WordList(['shit', 'many']),\n",
       " WordList(['many', 'girl']),\n",
       " WordList(['girl', 'front']),\n",
       " WordList(['front', 'cute']),\n",
       " WordList(['cute', 'want']),\n",
       " WordList(['want', 'loot']),\n",
       " WordList(['loot', 'tell']),\n",
       " WordList(['tell', 'lie']),\n",
       " WordList(['lie', 'swell']),\n",
       " WordList(['swell', 'guy']),\n",
       " WordList(['guy', 'peep']),\n",
       " WordList(['peep', 'thats']),\n",
       " WordList(['thats', 'weak']),\n",
       " WordList(['weak', 'shit']),\n",
       " WordList(['shit', 'thats']),\n",
       " WordList(['thats', 'run']),\n",
       " WordList(['run', 'game']),\n",
       " WordList(['game', 'every']),\n",
       " WordList(['every', 'freak']),\n",
       " WordList(['freak', 'get']),\n",
       " WordList(['get', 'born']),\n",
       " WordList(['born', 'oakland']),\n",
       " WordList(['oakland', 'grew']),\n",
       " WordList(['grew', 'pumpin']),\n",
       " WordList(['pumpin', 'short']),\n",
       " WordList(['short', 'know']),\n",
       " WordList(['know', 'rule']),\n",
       " WordList(['rule', 'flooze']),\n",
       " WordList(['flooze', 'who']),\n",
       " WordList(['who', 'short']),\n",
       " WordList(['short', 'ho']),\n",
       " WordList(['ho', 'smoked']),\n",
       " WordList(['smoked', 'blunts']),\n",
       " WordList(['blunts', 'yo']),\n",
       " WordList(['yo', 'tried']),\n",
       " WordList(['tried', 'get']),\n",
       " WordList(['get', 'looked']),\n",
       " WordList(['looked', 'front']),\n",
       " WordList(['front', 'door']),\n",
       " WordList(['door', 'shes']),\n",
       " WordList(['shes', 'forget']),\n",
       " WordList(['forget', 'every']),\n",
       " WordList(['every', 'ill']),\n",
       " WordList(['ill', 'hit']),\n",
       " WordList(['hit', 'happened']),\n",
       " WordList(['happened', 'girl']),\n",
       " WordList(['girl', 'didnt']),\n",
       " WordList(['didnt', 'mean']),\n",
       " WordList(['mean', 'relationship']),\n",
       " WordList(['relationship', 'hate']),\n",
       " WordList(['hate', 'dumb']),\n",
       " WordList(['dumb', 'dip']),\n",
       " WordList(['dip', 'really']),\n",
       " WordList(['really', 'cant']),\n",
       " WordList(['cant', 'trip']),\n",
       " WordList(['trip', 'ride']),\n",
       " WordList(['ride', 'tip']),\n",
       " WordList(['tip', 'didnt']),\n",
       " WordList(['didnt', 'mean']),\n",
       " WordList(['mean', 'chorus'])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(train['lyrics'][0]).ngrams(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4df4b8f27f3196f69a62740ad5f61f10370ea2b0"
   },
   "source": [
    "**3.2 Term frequency**\n",
    "Term frequency is simply the ratio of the count of a word present in a sentence, to the length of the sentence.\n",
    "\n",
    "Therefore, we can generalize term frequency as:\n",
    "\n",
    "TF = (Number of times term T appears in the particular row) / (number of terms in that row)\n",
    "\n",
    "To understand more about Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "28aaa149193a417d4d7bcfccd69f72e5a039b567"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>tf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>girl</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>something</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>every</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>shes</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>got</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ask</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sends</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>think</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>boy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>antmarco</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>curl</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>old</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pearl</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>someone</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>world</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>eye</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>id</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>somehing</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>mine</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>tonight</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>shine</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>adam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>right</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>tip</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>gem</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>youre</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>know</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>would</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>treat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>star</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>feeling</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>wish</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>wouldnt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>pirroni</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>wont</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>relax</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>smile</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>let</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>three</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        words  tf\n",
       "0        girl  17\n",
       "1   something  13\n",
       "2       every   9\n",
       "3        shes   3\n",
       "4         got   3\n",
       "5         ask   3\n",
       "6       sends   1\n",
       "7       think   1\n",
       "8         boy   1\n",
       "9    antmarco   1\n",
       "10       curl   1\n",
       "11        old   1\n",
       "12      pearl   1\n",
       "13    someone   1\n",
       "14      world   1\n",
       "15        eye   1\n",
       "16         id   1\n",
       "17   somehing   1\n",
       "18       mine   1\n",
       "19    tonight   1\n",
       "20      shine   1\n",
       "21       adam   1\n",
       "22      right   1\n",
       "23        tip   1\n",
       "24      white   1\n",
       "25        gem   1\n",
       "26      youre   1\n",
       "27       know   1\n",
       "28      would   1\n",
       "29      treat   1\n",
       "30       star   1\n",
       "31    feeling   1\n",
       "32       wish   1\n",
       "33    wouldnt   1\n",
       "34    pirroni   1\n",
       "35       wont   1\n",
       "36      relax   1\n",
       "37      smile   1\n",
       "38        let   1\n",
       "39      three   1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf1 = (train['lyrics'][1:2]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\n",
    "tf1.columns = ['words','tf']\n",
    "tf1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "43b5cec30702b42d5f67ebe689180cc109e7167f"
   },
   "source": [
    "**3.3 Inverse Document Frequency**\n",
    "The intuition behind inverse document frequency (IDF) is that a word is not of much use to us if it’s appearing in all the documents.\n",
    "\n",
    "Therefore, the IDF of each word is the log of the ratio of the total number of rows to the number of rows in which that word is present.\n",
    "\n",
    "IDF = log(N/n), where, N is the total number of rows and n is the number of rows in which the word was present.\n",
    "\n",
    "So, let’s calculate IDF for the same tweets for which we calculated the term frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "73d7a2e98ae53fe15bd98d2bd45cb0453377ff98"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>tf</th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>girl</td>\n",
       "      <td>17</td>\n",
       "      <td>1.851509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>something</td>\n",
       "      <td>13</td>\n",
       "      <td>2.396896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>every</td>\n",
       "      <td>9</td>\n",
       "      <td>1.081755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>shes</td>\n",
       "      <td>3</td>\n",
       "      <td>2.733368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>got</td>\n",
       "      <td>3</td>\n",
       "      <td>0.944176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ask</td>\n",
       "      <td>3</td>\n",
       "      <td>2.375156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sends</td>\n",
       "      <td>1</td>\n",
       "      <td>6.214608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>think</td>\n",
       "      <td>1</td>\n",
       "      <td>1.523260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>boy</td>\n",
       "      <td>1</td>\n",
       "      <td>2.137071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>antmarco</td>\n",
       "      <td>1</td>\n",
       "      <td>6.907755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>curl</td>\n",
       "      <td>1</td>\n",
       "      <td>5.521461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>old</td>\n",
       "      <td>1</td>\n",
       "      <td>1.030019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pearl</td>\n",
       "      <td>1</td>\n",
       "      <td>5.521461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>someone</td>\n",
       "      <td>1</td>\n",
       "      <td>2.577022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>world</td>\n",
       "      <td>1</td>\n",
       "      <td>1.594549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>eye</td>\n",
       "      <td>1</td>\n",
       "      <td>1.532477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>id</td>\n",
       "      <td>1</td>\n",
       "      <td>0.481267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>somehing</td>\n",
       "      <td>1</td>\n",
       "      <td>6.907755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>mine</td>\n",
       "      <td>1</td>\n",
       "      <td>2.419119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>tonight</td>\n",
       "      <td>1</td>\n",
       "      <td>2.453408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>shine</td>\n",
       "      <td>1</td>\n",
       "      <td>3.015935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>adam</td>\n",
       "      <td>1</td>\n",
       "      <td>5.521461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>right</td>\n",
       "      <td>1</td>\n",
       "      <td>1.294627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>tip</td>\n",
       "      <td>1</td>\n",
       "      <td>3.540459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "      <td>3.270169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>gem</td>\n",
       "      <td>1</td>\n",
       "      <td>5.115996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>youre</td>\n",
       "      <td>1</td>\n",
       "      <td>1.220780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>know</td>\n",
       "      <td>1</td>\n",
       "      <td>0.619897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>would</td>\n",
       "      <td>1</td>\n",
       "      <td>1.801810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>treat</td>\n",
       "      <td>1</td>\n",
       "      <td>3.649659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>star</td>\n",
       "      <td>1</td>\n",
       "      <td>1.496109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>feeling</td>\n",
       "      <td>1</td>\n",
       "      <td>2.407946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>wish</td>\n",
       "      <td>1</td>\n",
       "      <td>2.673649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>wouldnt</td>\n",
       "      <td>1</td>\n",
       "      <td>3.411248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>pirroni</td>\n",
       "      <td>1</td>\n",
       "      <td>6.907755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>wont</td>\n",
       "      <td>1</td>\n",
       "      <td>1.714798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>relax</td>\n",
       "      <td>1</td>\n",
       "      <td>4.342806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>smile</td>\n",
       "      <td>1</td>\n",
       "      <td>2.847312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>let</td>\n",
       "      <td>1</td>\n",
       "      <td>1.075873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>three</td>\n",
       "      <td>1</td>\n",
       "      <td>3.296837</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        words  tf       idf\n",
       "0        girl  17  1.851509\n",
       "1   something  13  2.396896\n",
       "2       every   9  1.081755\n",
       "3        shes   3  2.733368\n",
       "4         got   3  0.944176\n",
       "5         ask   3  2.375156\n",
       "6       sends   1  6.214608\n",
       "7       think   1  1.523260\n",
       "8         boy   1  2.137071\n",
       "9    antmarco   1  6.907755\n",
       "10       curl   1  5.521461\n",
       "11        old   1  1.030019\n",
       "12      pearl   1  5.521461\n",
       "13    someone   1  2.577022\n",
       "14      world   1  1.594549\n",
       "15        eye   1  1.532477\n",
       "16         id   1  0.481267\n",
       "17   somehing   1  6.907755\n",
       "18       mine   1  2.419119\n",
       "19    tonight   1  2.453408\n",
       "20      shine   1  3.015935\n",
       "21       adam   1  5.521461\n",
       "22      right   1  1.294627\n",
       "23        tip   1  3.540459\n",
       "24      white   1  3.270169\n",
       "25        gem   1  5.115996\n",
       "26      youre   1  1.220780\n",
       "27       know   1  0.619897\n",
       "28      would   1  1.801810\n",
       "29      treat   1  3.649659\n",
       "30       star   1  1.496109\n",
       "31    feeling   1  2.407946\n",
       "32       wish   1  2.673649\n",
       "33    wouldnt   1  3.411248\n",
       "34    pirroni   1  6.907755\n",
       "35       wont   1  1.714798\n",
       "36      relax   1  4.342806\n",
       "37      smile   1  2.847312\n",
       "38        let   1  1.075873\n",
       "39      three   1  3.296837"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i,word in enumerate(tf1['words']):\n",
    "  tf1.loc[i, 'idf'] = np.log(train.shape[0]/(len(train[train['lyrics'].str.contains(word)])))\n",
    "\n",
    "tf1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4ee14ef4075c44fd6fe6dc5b3448ef55bf87bded"
   },
   "source": [
    "The more the value of IDF, the more unique is the word.\n",
    "\n",
    " \n",
    "\n",
    "**3.4 Term Frequency – Inverse Document Frequency (TF-IDF)**\n",
    "TF-IDF is the multiplication of the TF and IDF which we calculated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "59ebeb745bd8fa410cdef5c7e9af01371cdd46f9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>tf</th>\n",
       "      <th>idf</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>girl</td>\n",
       "      <td>17</td>\n",
       "      <td>1.851509</td>\n",
       "      <td>31.475661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>something</td>\n",
       "      <td>13</td>\n",
       "      <td>2.396896</td>\n",
       "      <td>31.159645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>every</td>\n",
       "      <td>9</td>\n",
       "      <td>1.081755</td>\n",
       "      <td>9.735797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>shes</td>\n",
       "      <td>3</td>\n",
       "      <td>2.733368</td>\n",
       "      <td>8.200104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>got</td>\n",
       "      <td>3</td>\n",
       "      <td>0.944176</td>\n",
       "      <td>2.832528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ask</td>\n",
       "      <td>3</td>\n",
       "      <td>2.375156</td>\n",
       "      <td>7.125467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sends</td>\n",
       "      <td>1</td>\n",
       "      <td>6.214608</td>\n",
       "      <td>6.214608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>think</td>\n",
       "      <td>1</td>\n",
       "      <td>1.523260</td>\n",
       "      <td>1.523260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>boy</td>\n",
       "      <td>1</td>\n",
       "      <td>2.137071</td>\n",
       "      <td>2.137071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>antmarco</td>\n",
       "      <td>1</td>\n",
       "      <td>6.907755</td>\n",
       "      <td>6.907755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>curl</td>\n",
       "      <td>1</td>\n",
       "      <td>5.521461</td>\n",
       "      <td>5.521461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>old</td>\n",
       "      <td>1</td>\n",
       "      <td>1.030019</td>\n",
       "      <td>1.030019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pearl</td>\n",
       "      <td>1</td>\n",
       "      <td>5.521461</td>\n",
       "      <td>5.521461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>someone</td>\n",
       "      <td>1</td>\n",
       "      <td>2.577022</td>\n",
       "      <td>2.577022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>world</td>\n",
       "      <td>1</td>\n",
       "      <td>1.594549</td>\n",
       "      <td>1.594549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>eye</td>\n",
       "      <td>1</td>\n",
       "      <td>1.532477</td>\n",
       "      <td>1.532477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>id</td>\n",
       "      <td>1</td>\n",
       "      <td>0.481267</td>\n",
       "      <td>0.481267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>somehing</td>\n",
       "      <td>1</td>\n",
       "      <td>6.907755</td>\n",
       "      <td>6.907755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>mine</td>\n",
       "      <td>1</td>\n",
       "      <td>2.419119</td>\n",
       "      <td>2.419119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>tonight</td>\n",
       "      <td>1</td>\n",
       "      <td>2.453408</td>\n",
       "      <td>2.453408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>shine</td>\n",
       "      <td>1</td>\n",
       "      <td>3.015935</td>\n",
       "      <td>3.015935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>adam</td>\n",
       "      <td>1</td>\n",
       "      <td>5.521461</td>\n",
       "      <td>5.521461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>right</td>\n",
       "      <td>1</td>\n",
       "      <td>1.294627</td>\n",
       "      <td>1.294627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>tip</td>\n",
       "      <td>1</td>\n",
       "      <td>3.540459</td>\n",
       "      <td>3.540459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "      <td>3.270169</td>\n",
       "      <td>3.270169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>gem</td>\n",
       "      <td>1</td>\n",
       "      <td>5.115996</td>\n",
       "      <td>5.115996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>youre</td>\n",
       "      <td>1</td>\n",
       "      <td>1.220780</td>\n",
       "      <td>1.220780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>know</td>\n",
       "      <td>1</td>\n",
       "      <td>0.619897</td>\n",
       "      <td>0.619897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>would</td>\n",
       "      <td>1</td>\n",
       "      <td>1.801810</td>\n",
       "      <td>1.801810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>treat</td>\n",
       "      <td>1</td>\n",
       "      <td>3.649659</td>\n",
       "      <td>3.649659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>star</td>\n",
       "      <td>1</td>\n",
       "      <td>1.496109</td>\n",
       "      <td>1.496109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>feeling</td>\n",
       "      <td>1</td>\n",
       "      <td>2.407946</td>\n",
       "      <td>2.407946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>wish</td>\n",
       "      <td>1</td>\n",
       "      <td>2.673649</td>\n",
       "      <td>2.673649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>wouldnt</td>\n",
       "      <td>1</td>\n",
       "      <td>3.411248</td>\n",
       "      <td>3.411248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>pirroni</td>\n",
       "      <td>1</td>\n",
       "      <td>6.907755</td>\n",
       "      <td>6.907755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>wont</td>\n",
       "      <td>1</td>\n",
       "      <td>1.714798</td>\n",
       "      <td>1.714798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>relax</td>\n",
       "      <td>1</td>\n",
       "      <td>4.342806</td>\n",
       "      <td>4.342806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>smile</td>\n",
       "      <td>1</td>\n",
       "      <td>2.847312</td>\n",
       "      <td>2.847312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>let</td>\n",
       "      <td>1</td>\n",
       "      <td>1.075873</td>\n",
       "      <td>1.075873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>three</td>\n",
       "      <td>1</td>\n",
       "      <td>3.296837</td>\n",
       "      <td>3.296837</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        words  tf       idf      tfidf\n",
       "0        girl  17  1.851509  31.475661\n",
       "1   something  13  2.396896  31.159645\n",
       "2       every   9  1.081755   9.735797\n",
       "3        shes   3  2.733368   8.200104\n",
       "4         got   3  0.944176   2.832528\n",
       "5         ask   3  2.375156   7.125467\n",
       "6       sends   1  6.214608   6.214608\n",
       "7       think   1  1.523260   1.523260\n",
       "8         boy   1  2.137071   2.137071\n",
       "9    antmarco   1  6.907755   6.907755\n",
       "10       curl   1  5.521461   5.521461\n",
       "11        old   1  1.030019   1.030019\n",
       "12      pearl   1  5.521461   5.521461\n",
       "13    someone   1  2.577022   2.577022\n",
       "14      world   1  1.594549   1.594549\n",
       "15        eye   1  1.532477   1.532477\n",
       "16         id   1  0.481267   0.481267\n",
       "17   somehing   1  6.907755   6.907755\n",
       "18       mine   1  2.419119   2.419119\n",
       "19    tonight   1  2.453408   2.453408\n",
       "20      shine   1  3.015935   3.015935\n",
       "21       adam   1  5.521461   5.521461\n",
       "22      right   1  1.294627   1.294627\n",
       "23        tip   1  3.540459   3.540459\n",
       "24      white   1  3.270169   3.270169\n",
       "25        gem   1  5.115996   5.115996\n",
       "26      youre   1  1.220780   1.220780\n",
       "27       know   1  0.619897   0.619897\n",
       "28      would   1  1.801810   1.801810\n",
       "29      treat   1  3.649659   3.649659\n",
       "30       star   1  1.496109   1.496109\n",
       "31    feeling   1  2.407946   2.407946\n",
       "32       wish   1  2.673649   2.673649\n",
       "33    wouldnt   1  3.411248   3.411248\n",
       "34    pirroni   1  6.907755   6.907755\n",
       "35       wont   1  1.714798   1.714798\n",
       "36      relax   1  4.342806   4.342806\n",
       "37      smile   1  2.847312   2.847312\n",
       "38        let   1  1.075873   1.075873\n",
       "39      three   1  3.296837   3.296837"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf1['tfidf'] = tf1['tf'] * tf1['idf']\n",
    "tf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "bbb238b6b88039e989dc7bc31d72bd09fef0aae5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
