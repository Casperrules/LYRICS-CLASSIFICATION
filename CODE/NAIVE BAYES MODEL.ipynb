{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LYRICS MOOD CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>artist</th>\n",
       "      <th>title</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>genre</th>\n",
       "      <th>mood</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAAAAW128F429D538.h5</td>\n",
       "      <td>Casual</td>\n",
       "      <td>I Didn't Mean To</td>\n",
       "      <td>Verse One:\\n\\nAlright I might\\nHave had a litt...</td>\n",
       "      <td>Hip Hop/Rap</td>\n",
       "      <td>sad</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAAAEF128F4273421.h5</td>\n",
       "      <td>Adam Ant</td>\n",
       "      <td>Something Girls</td>\n",
       "      <td>Adam Ant/Marco Pirroni\\nEvery girl is a someth...</td>\n",
       "      <td>Rock</td>\n",
       "      <td>happy</td>\n",
       "      <td>1982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAAAFD128F92F423A.h5</td>\n",
       "      <td>Gob</td>\n",
       "      <td>Face the Ashes</td>\n",
       "      <td>I've just erased it's been a while, I've got a...</td>\n",
       "      <td>Rock</td>\n",
       "      <td>sad</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAABJV128F1460C49.h5</td>\n",
       "      <td>Lionel Richie</td>\n",
       "      <td>Tonight Will Be Alright</td>\n",
       "      <td>Little darling \\nWhere you've been so long \\nI...</td>\n",
       "      <td>R&amp;B</td>\n",
       "      <td>happy</td>\n",
       "      <td>1986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAABLR128F423B7E3.h5</td>\n",
       "      <td>Blue Rodeo</td>\n",
       "      <td>Floating</td>\n",
       "      <td>Lead Vocal by Greg\\n\\nWell, these late night c...</td>\n",
       "      <td>Rock</td>\n",
       "      <td>sad</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    file         artist                    title  \\\n",
       "0  TRAAAAW128F429D538.h5         Casual         I Didn't Mean To   \n",
       "1  TRAAAEF128F4273421.h5       Adam Ant          Something Girls   \n",
       "2  TRAAAFD128F92F423A.h5            Gob           Face the Ashes   \n",
       "3  TRAABJV128F1460C49.h5  Lionel Richie  Tonight Will Be Alright   \n",
       "4  TRAABLR128F423B7E3.h5     Blue Rodeo                 Floating   \n",
       "\n",
       "                                              lyrics        genre   mood  year  \n",
       "0  Verse One:\\n\\nAlright I might\\nHave had a litt...  Hip Hop/Rap    sad  1994  \n",
       "1  Adam Ant/Marco Pirroni\\nEvery girl is a someth...         Rock  happy  1982  \n",
       "2  I've just erased it's been a while, I've got a...         Rock    sad  2007  \n",
       "3  Little darling \\nWhere you've been so long \\nI...          R&B  happy  1986  \n",
       "4  Lead Vocal by Greg\\n\\nWell, these late night c...         Rock    sad  1987  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../../dataset/training/train_lyrics_1000.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: ['sad' 'happy' 'sad' 'happy' 'sad'] ...\n",
      "after: [1 0 1 0 1] ...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "X_train = df['lyrics'].values \n",
    "\n",
    "y_train = df['mood'].values\n",
    "\n",
    "print('before: %s ...' %y_train[:5])\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(y_train)\n",
    "y_train = le.transform(y_train)\n",
    "\n",
    "print('after: %s ...' %y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save object to disk\n",
    "\n",
    "import pickle\n",
    "\n",
    "pickle_out = open('./lyrics_label_encoder_py1.pkl', 'wb')\n",
    "pickle.dump(le, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction: Word counts and Vectorizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porter Stemmer\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "\n",
    "porter_stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "def porter_tokenizer(text, stemmer=porter_stemmer):\n",
    "    \"\"\"\n",
    "    A Porter-Stemmer-Tokenizer hybrid to splits sentences into words (tokens) \n",
    "    and applies the porter stemming algorithm to each of the obtained token. \n",
    "    Tokens that are only consisting of punctuation characters are removed as well.\n",
    "    Only tokens that consist of more than one letter are being kept.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        \n",
    "    text : `str`. \n",
    "      A sentence that is to split into words.\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    \n",
    "    no_punct : `str`. \n",
    "      A list of tokens after stemming and removing Sentence punctuation patterns.\n",
    "    \n",
    "    \"\"\"\n",
    "    lower_txt = text.lower()\n",
    "    tokens = nltk.wordpunct_tokenize(lower_txt)\n",
    "    stems = [porter_stemmer.stem(t) for t in tokens]\n",
    "    no_punct = [s for s in stems if re.match('^[a-zA-Z]+$', s) is not None]\n",
    "    return no_punct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['don', 't', 'want', 'swim']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter_tokenizer(\"Don't !!! --- want swimming. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop words ['i', 'me', 'my', 'myself', 'we'] ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "with open('./stopwords_eng.txt', 'r') as infile:\n",
    "    stop_words = infile.read().splitlines()\n",
    "print('stop words %s ...' %stop_words[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Vectorizer\n",
    "#Convert a collection of text documents to a matrix of token counts\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer(\n",
    "            encoding='utf-8',\n",
    "            decode_error='replace',\n",
    "            strip_accents='unicode',\n",
    "            analyzer='word',\n",
    "            binary=False,\n",
    "            stop_words=stop_words,\n",
    "            tokenizer=porter_tokenizer,\n",
    "            ngram_range=(1,1)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST:\n",
      "Vocabulary: ['didn', 'like', 'swim', 'swimmer']\n",
      "Sentence 1: [[0 1 1 1]]\n",
      "Sentence 2: [[0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "vocab = [\"123 1 The\\n swimmer likes swimming so he swims. Don't didn`t\"]\n",
    "\n",
    "vec = vec.fit(vocab)\n",
    "\n",
    "sentence1 = vec.transform([u'The swimmer likes swimming.'])\n",
    "sentence2 = vec.transform(['The\\nswimmer \\nswims.'])\n",
    "\n",
    "\n",
    "print('TEST:')\n",
    "print('Vocabulary: %s' %vec.get_feature_names())\n",
    "print('Sentence 1: %s' %sentence1.toarray())\n",
    "print('Sentence 2: %s' %sentence2.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting the lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = vec.fit(X_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 8550\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary size: %s' %len(vec.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 2 (N-grams = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST:\n",
      "Vocabulary: ['like swim', 'swim didn', 'swim swim', 'swimmer like']\n",
      "Sentence 1: [[1 0 0 1]]\n",
      "Sentence 2: [[0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(\n",
    "            encoding='utf-8',\n",
    "            decode_error='replace',\n",
    "            strip_accents='unicode',\n",
    "            analyzer='word',\n",
    "            binary=False,\n",
    "            stop_words=stop_words,\n",
    "            tokenizer=porter_tokenizer,\n",
    "            ngram_range=(2,2)\n",
    "    )\n",
    "\n",
    "vocab = [\"123 1 The\\n swimmer likes swimming so he swims. Don't didn`t\"]\n",
    "\n",
    "vec = vec.fit(vocab)\n",
    "\n",
    "sentence1 = vec.transform([u'The swimmer likes swimming.'])\n",
    "sentence2 = vec.transform(['The\\nswimmer \\nswims.'])\n",
    "\n",
    "\n",
    "print('TEST:')\n",
    "print('Vocabulary: %s' %vec.get_feature_names())\n",
    "print('Sentence 1: %s' %sentence1.toarray())\n",
    "print('Sentence 2: %s' %sentence2.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfidf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "            encoding='utf-8',\n",
    "            decode_error='replace',\n",
    "            strip_accents='unicode',\n",
    "            analyzer='word',\n",
    "            binary=False,\n",
    "            stop_words=stop_words,\n",
    "            tokenizer=porter_tokenizer\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST:\n",
      "Vocabulary: ['didn', 'like', 'swim', 'swimmer']\n",
      "Sentence 1: [[0.         0.57735027 0.57735027 0.57735027]]\n",
      "Sentence 2: [[0.         0.         0.70710678 0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "vocab = [\"123 1 The\\n swimmer likes swimming so he swims. Don't didn`t\"]\n",
    "\n",
    "tfidf = tfidf.fit(vocab)\n",
    "\n",
    "sentence1 = tfidf.transform([u'The swimmer likes swimming.'])\n",
    "sentence2 = tfidf.transform(['The\\nswimmer \\nswims.'])\n",
    "\n",
    "\n",
    "print('TEST:')\n",
    "print('Vocabulary: %s' %tfidf.get_feature_names())\n",
    "print('Sentence 1: %s' %sentence1.toarray())\n",
    "print('Sentence 2: %s' %sentence2.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting the lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 8550\n"
     ]
    }
   ],
   "source": [
    "tfidf = tfidf.fit(X_train.ravel())\n",
    "\n",
    "print('Vocabulary size: %s' %len(tfidf.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models: Multivariate Bernoulli and Multinomial naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance metric: F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom scorer methods to account for positive-negative class labels\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "# `pos_label` for positive class, since we have sad=1, happy=0\n",
    "\n",
    "f1_scorer = metrics.make_scorer(metrics.f1_score, greater_is_better=True, pos_label=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MIRTUNJAY\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\MIRTUNJAY\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'clf']\n",
      "parameters:\n",
      "{'vect__binary': [True],\n",
      " 'vect__ngram_range': [(...), (...), (...)],\n",
      " 'vect__stop_words': [[...], None],\n",
      " 'vect__tokenizer': [<function porter_tokenizer at 0x000001F1D75A5950>, None]}\n",
      "Fitting 10 folds for each of 12 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed:  9.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.617\n",
      "Best parameters set:\n",
      "\tvect__binary: True\n",
      "\tvect__ngram_range: (1, 1)\n",
      "\tvect__stop_words: ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n",
      "\tvect__tokenizer: <function porter_tokenizer at 0x000001F1D75A5950>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from pprint import pprint\n",
    "\n",
    "pipeline_1 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', BernoulliNB())\n",
    "])\n",
    "\n",
    "parameters_1 = dict(\n",
    "    vect__binary=[True],\n",
    "    vect__stop_words=[stop_words, None],\n",
    "    vect__tokenizer=[porter_tokenizer, None],\n",
    "    vect__ngram_range=[(1,1), (2,2), (3,3)],\n",
    ")\n",
    "\n",
    "grid_search_1 = GridSearchCV(pipeline_1, \n",
    "                           parameters_1, \n",
    "                           n_jobs=1, \n",
    "                           verbose=1,\n",
    "                           scoring=f1_scorer,\n",
    "                           cv=10\n",
    "                )\n",
    "\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline_1.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters_1, depth=2)\n",
    "grid_search_1.fit(X_train, y_train)\n",
    "print(\"Best score: %0.3f\" % grid_search_1.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters_1 = grid_search_1.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters_1.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters_1[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'clf']\n",
      "parameters:\n",
      "{'vect__binary': [False],\n",
      " 'vect__ngram_range': [(...), (...), (...)],\n",
      " 'vect__stop_words': [[...], None],\n",
      " 'vect__tokenizer': [<function porter_tokenizer at 0x000001F1D75A5950>, None]}\n",
      "Fitting 10 folds for each of 12 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed:  9.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.620\n",
      "Best parameters set:\n",
      "\tvect__binary: False\n",
      "\tvect__ngram_range: (1, 1)\n",
      "\tvect__stop_words: ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n",
      "\tvect__tokenizer: <function porter_tokenizer at 0x000001F1D75A5950>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "pipeline_3 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "parameters_3 = dict(\n",
    "    vect__binary=[False],\n",
    "    vect__stop_words=[stop_words, None],\n",
    "    vect__tokenizer=[porter_tokenizer, None],\n",
    "    vect__ngram_range=[(1,1), (2,2), (3,3)],\n",
    ")\n",
    "\n",
    "grid_search_3 = GridSearchCV(pipeline_3, \n",
    "                           parameters_3, \n",
    "                           n_jobs=1, \n",
    "                           verbose=1,\n",
    "                           scoring=f1_scorer,\n",
    "                           cv=10\n",
    "                )\n",
    "\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline_3.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters_3, depth=2)\n",
    "grid_search_3.fit(X_train, y_train)\n",
    "print(\"Best score: %0.3f\" % grid_search_3.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters_3 = grid_search_3.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters_3.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters_3[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'clf']\n",
      "parameters:\n",
      "{'vect__binary': [False],\n",
      " 'vect__ngram_range': [(...), (...), (...)],\n",
      " 'vect__stop_words': [[...], None],\n",
      " 'vect__tokenizer': [<function porter_tokenizer at 0x0000011E4110EBF8>, None]}\n",
      "Fitting 10 folds for each of 12 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed: 11.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.335\n",
      "Best parameters set:\n",
      "\tvect__binary: False\n",
      "\tvect__ngram_range: (2, 2)\n",
      "\tvect__stop_words: ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n",
      "\tvect__tokenizer: None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "pipeline_4 = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "parameters_4 = dict(\n",
    "    vect__binary=[False],\n",
    "    vect__stop_words=[stop_words, None],\n",
    "    vect__tokenizer=[porter_tokenizer, None],\n",
    "    vect__ngram_range=[(1,1), (2,2), (3,3)],\n",
    ")\n",
    "\n",
    "grid_search_4 = GridSearchCV(pipeline_4, \n",
    "                           parameters_4, \n",
    "                           n_jobs=1, \n",
    "                           verbose=1,\n",
    "                           scoring=f1_scorer,\n",
    "                           cv=10\n",
    "                )\n",
    "\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline_4.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters_4, depth=2)\n",
    "grid_search_4.fit(X_train, y_train)\n",
    "print(\"Best score: %0.3f\" % grid_search_4.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters_4 = grid_search_4.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters_4.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters_4[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "  ...se_idf=True, vocabulary=None)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_clf = Pipeline([\n",
    "                ('vect', TfidfVectorizer(\n",
    "                                         binary=False,\n",
    "                                         stop_words=stop_words,\n",
    "                                         tokenizer=porter_tokenizer,\n",
    "                                         ngram_range=(1,1),\n",
    "                                         )\n",
    "                ),\n",
    "                ('clf', MultinomialNB(alpha=1.0)),\n",
    "               ])\n",
    "final_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba =max(final_clf.predict_proba(X_valid)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Where, oh, where have you been, my love?\\nWhere, oh, where can you be?\\nIt's been so long since the moon has gone\\nAnd, oh, what a wreck you've made me\\n\\nAre you there over the ocean?\\nAre you there up in the sky?\\nUntil the return of my love\\nThis lullaby\\n\\nMy hope is on the horizon\\nEvery face, your eyes I can see\\nI plead and I pray through each night and day\\nOur embrace is only a dream\\n\\nAnd as sure as days come from moments\\nEach hour becomes a life's time\\nWhen she'd left I'd only begun\\nThis lullaby\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the prediction of lyrics at position 1 is sad with the probability of 0.7092180010988367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MIRTUNJAY\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "label = final_clf.predict(X_valid)[0]\n",
    "print(\"the prediction of lyrics at position 1 is \"+le.inverse_transform(label)+\" with the probability of \"+str(proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"When it happened \\nsomething snapped inside \\nmade me want to hide \\nall alone on my own \\nall alone on my own \\nI stood up on the stand \\nwith my eyes shut tight \\ndidn't want to see anbody \\nfeeling happy\\nhaving a good time, now hey \\ndoing alright doing alright, \\ndoing alright doing alright\\n\\nLooked hard into the dancing crowd \\nfelt like screaming out loud \\nI saw you standing in there \\nI saw your long \\nsaw your long hair\\nopened up my eyes, baby \\nyou made me\\nrealize all I want to do \\nall I want to do now, girl \\nis look at you looking at you baby,\\nlook at you, looking at you baby \\nyeah, yeah, hey \\n\\nLooked hard into the dancing crowd \\nI felt like screaming out loud\\nall I wanna do all I wanna do\\nall I want to do\\nis look at you looking at you, baby\\nlook at you, looking at you baby\\nlooking at you looking \\nat you looking at you, baby \\nyou baby you baby you baby, yeah\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_clf.predict(X_valid)[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x144 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "cm = metrics.confusion_matrix(y_train, final_clf.predict(X_train))\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "mpl.rc(\"figure\", figsize=(4, 2))\n",
    "\n",
    "hm = sns.heatmap(cm, \n",
    "            cbar=False,\n",
    "            annot=True, \n",
    "            square=True,\n",
    "            fmt='d',\n",
    "            yticklabels=['happy','sad'],\n",
    "            xticklabels=['happy','sad'],\n",
    "            cmap='Blues'\n",
    "            )\n",
    "plt.title('Confusion matrix - Training dataset')\n",
    "plt.ylabel('actual class')\n",
    "plt.xlabel('predicted class')\n",
    "plt.tight_layout()\n",
    "#plt.savefig('./images/confmat_training11.eps', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../dataset/validation/valid_lyrics_200.csv')\n",
    "\n",
    "X_valid = df['lyrics'].values \n",
    "y_valid = df['mood'].values\n",
    "\n",
    "y_valid = le.transform(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPAAAACICAYAAADZJGY+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAGBRJREFUeJztnXe0FdXZh5/fpTcpgi1BBOwYbNgQFRUNFgIaICoRQSOiKLEkphkCGo1G4zLGLhpRExW7wRL9EhVFoyiKgCI2jEoRCL1Ie78/9j46XG8599xz7r1neJ+1Zp2ZPbu8M2d+s8vsIjPDcZzipKS2DXAcJ3dcwI5TxLiAHaeIcQE7ThHjAnacIsYF7DhFTK0KWFITSf+QtFTSg9WIZ5CkZ/NpW20h6RBJ79e2HWUhabakXnH/15LGZuM3h3Rq7B5Ux866QFYClnSKpDckrZA0V9LTknrkIf3+wNbAlmY2INdIzOxvZnZ0HuwpKJJM0o4V+TGzl8xslwKk/StJE8twbytpraQ9qhKfmV1hZj/Jk22b3JdC3YPqks3/V9PpVCpgSRcC1wFXEMS2PXAT0Lc6RkY6ALPMbH0e4ip6JNUvYPT3AN0ldSzlfhIwzcymFzBtp1CYWbkb0BJYAQyowE8jgsDnxO06oFE81xP4HLgI+BKYCwyN58YAa4F1MY0zgNHAvYm4dwAMqB+PhwAfA8uBT4BBCfeXE+G6A5OBpfG3e+LcC8BlwKQYz7NA23KuLWP/xQn7+wHHArOA/wG/TvjfH3gVWBL93gA0jOcmxmtZGa/3R4n4fwHMI4isJ/B5DNM5prFPPN4OWAj0rOh/q+C/ehYYVcrtdWBkIr1/A4tiOn8DWiX8zgZ6xf3S/9WpwKcx7G9K+c3pviTi3i3+b0uAGcAPEufuAm4Enoz/52tA5wruQT7tbA1MABYAi+P+dxNpDaGM5zWeOx14L4b7J9ChvHQq/E8r+cN7A+uJAirHz6XAf4CtgHbAK8BlCQGsj34aEB78VUDrch6C0sc7xIupDzQDlgG7xHPbAl1KCxhoE2/KqTHcyfF4y4SAPwJ2BprE4ysrEPB6YFS0/8z4Z/0daAF0AdYAnaL/fYEDY7o7xD/o/ER8BuxYRvxXEV6ETfj2w3tmjKdp/KOvyUW8Ma5BwAeJ410IL9F28XhH4KhoS7v4MF1XmYCB3ePDdmgMe228rl7VuC+Zl1gD4EPg10BD4AiCIDLPwV2El9z+Mf6/AfeXc/35tnNL4Ifxv2kBPAg8Fs9V9Lz2i9e0W0zrEuCV8tKpjoAHAfMq8fMRcGzi+PvA7MQfsZrEC4CQkx2Yo4CXxBvWpJQNQ/hGwKcCr5c6/yowJCHgSxLnzgGeqUDAq4F68bhFtOeAhJ83gX7lhD8feLSSB3Ut0Lishzfh9gQwDXiHWLrJUcBN40PVPR5fDjxegf9+wFtZCHgUCdHE/2ptxm+O9yUj4EMIpZOSxPn7gNEJAY9NnDsWmFlOunm1swz/ewGLE3GX97w+DZyROC4hZGwdskknuVVWB14EtK2kbrYdoUiS4dPo9nUctmkddxXQvJJ0v4WZrSQUW4YDcyU9KWnXLOzJ2PSdxPG8KtizyMw2xP3V8Xd+4vzqTHhJO0uaIGmepGWEdoO2FcQNsMDM1lTi53ZgD+AvZvZVWR5iS/yKuD1dlh8zW0XIJQZLEuEFPS4Rx1aS7pf0RbT/3izsh3DPP0uks5Lw7GTizeW+bBK3mW1MuOX6f+bVTklNJd0q6dPofyLQSlK9Sp7XDsCfJS2RtIRQglCpa8qKygT8KqGI2K8CP3OiQRm2j265sJKQS2TYJnnSzP5pZkcRiiMzCQ92ZfZkbPoiR5uqws0Eu3Yysy0IxT5VEsYqOimpOaFd4Q5gtKQ2ZUYSWuKbx+2YCqIcBwwkFJVbEOptGf4Q7eka7f9xFvZDqC+2T9jclFC8zJDLfckwB2gvKfms5vp/5tvOiwjVkAOi/0MzUUOFz+tnwFlm1iqxNTGzV6p6QRUK2MyWEoodN0rqF984DSQdI+mP0dt9wCWS2klqG/3fW1VDIm8Dh0raXlJL4FeZE5K2lvQDSc2Arwh1mQ1lxPEUsHP89FVf0o8IdZ8JZfjNNy0IRdQV8W17dqnz84FOVYzzz8CbFj7ZPAncUk0bXyIU7W4jFCfXJs61INzXJZK+A/w8yzgfAo6X1ENSQ0KbR/LZqs59eY3wYr84Pns9gT7A/VnaVkg7WxBKYEvii/V3mROVPK+3AL+S1CX6bSkp+Rk16+ek0s9IZnYtcCGhor2A8PY4F3gsevk98AahfjYNmBLdqoyZPQc8EON6k01FV0J4480hFDkOI9RfS8exCDg++l1EaEE+3swW5mJTFfkZcAqhkeV2wrUkGQ2Mi0WngZVFJqkvoSFxeHS6ENhH0qBcDbRQybqbUEq5u9TpMcA+hNb7J4FHsoxzBjCC0Lg3l9Bo+HnCS873Jb5gfgAcQ2gZvwkYbGYzs7GtwHZeR2h4XEhoyH0m4bfc59XMHiU0XN4fi97T4/WVl065KFaaHccpQrwvtOMUMS5gxyliXMCOU8S4gB2niClk5/nNilnzVtXJ1sA9+/+htk0ok9UvX5btd2CnAjwHdpwixgXsOEWMC9hxihgXsOMUMS5gxyliXMCOU8S4gB2niHEBO04R4wJ2nCImdQKW1FlSo7jfU9JISa1q2y7HKQSpEzDwMLAhTox9B9CRMIDbcVJHGgW8MU6idwJhStQLCHMSOU7qSKOA10k6GTiNb6bkaVCL9jhOwUijgIcCBwGXm9kncSmRXCfZc5w6TeqGE5rZu8BIAEmtgRZmdmXtWlU+f75yNJNfnUjL1m248a6Hvnb/x8P38eSjD1BSrx77HXgIQ88+v0btOm/gQQzp0w0zY8bH8xl2xaMcuEd7/jCiNw0b1OOt9+cw/MrH2LBhY+WROQUjdTmwpBckbRGn+ZwK/FXStbVtV3kceUwfRl994yZu70yZzGuTXuAvd47npnEPc8JJg2vUpu3atuCc/gdx8Bk3023wDdQrKeFHR3Vl7G9+yODR4+k2+Ab+O28JP+69V43a5Xyb1AkYaGlmy4ATgb+a2b5AnV3/dY8996VFi5abuD31+IP0P2UoDRo2BKBV6zLnci8o9euV0KRRA+rF31Wr1/LVuvV8+FlYyODfkz+iX88uNW6XsylpFHB9SdsSVh+o0mTuks6Nxe5aZc7nnzLjnbe4aPip/HLkGcx6b0bNpr9wOdfd/zKzHr6ITx67mGUr1/DQv6fToH499tklrJpzwuFd+O5WLSuJySk0aRTwpYRV/D40s8mSOgEfZBl2G2CypPGSesf1g8pF0jCFhc/feOCeO6tp9jds2LCBFcuXcc3Nd3P62Rdw1eiLqcn5u1u1aMzxPXZjt4HX0qnfH2nWuCEnHb0ng383nj+OPIaXbjuL5au+Yr3Xf2udNDZiPUhYwCtz/DFhhbhswl4i6bfA0YTW7BskjQfuMLOPyvB/G2GJkrzOidW23dZ0P/RIJLHzbntQUlLCsqWLadmqZorSR3TrzOy5i1m4ZBUAj018lwO/1577n51KrxF3AHDkfp3ZqX2265M5hSJ1ObCkxpJGSLpJ0p2ZLdvwcemReXFbT1jE+aHEWlAF58AePZk65XUAvvjsU9avW8cWLWuuZP/Z/KXs36U9TRqFz+eH79uJ92cvoF2rZgA0bFCPiwYdwu2PvV5jNjllk7ocmLDK/UzCOsWXEpbQfC+bgJJGEjqALATGAj83s3VxZbwPCOss5ZWrx/ySaW+/ybKlSxjS//ucMnQ4vY7tx/VXjWbEkP7Ur9+A8399KZWU5vPK5Hc/59HnZ/DqnWezfsNGps6ayx1PvMHoM3txTPddKCkRtz/6Oi9O+aTGbHLKJnVrI0l6y8z2lvSOmXWV1AD4p5kdkUXYSwnF5dLrCyNpNzMr90Xg08pWDZ9WNj+kMQdeF3+XSNqDUBTeIZuAZjZK0j5xVUADJpnZlHguq1zccWqS1NWBgdvip6DfAk8A7wJZ1V9jA9Y4wqLPbQmdQC4plKGOU11SlwOb2di4+yJVX0z7FGBvM1sDIOlKqrHeseMUmtQIWNKFFZ2PC5VXxmygMbAmHjcCvvX5yHHqCqkRMNAiD3F8BcyQ9ByhDnwU8LKk6wHMbGQe0nCcvJEaAZvZmDxE82jcMryQhzgdp2CkRsAZJI0DfmpmS+Jxa+BPZnZ6ZWHNbJykhsCuhBz4fTNbW1CDHacapE7AQNeMeAHMbLGkvbMJKOlY4FZCvVdAR0lnmdnThTHVcapHGgVcIqm1mS0GiOOCs73Oa4HDzezDGLYz8CTgAnbqJGkU8J+AVyQ9RCgGDwQuzzLslxnxRj4GvsyzfY6TN1InYDO7W9IbwBGEYvCJcZqdbJgh6SlgPEH8AwjDC0+McT9SCJsdJ1dSJ2D4el6sbEWbpDEwHzgsHi8A2gB9CIJ2ATt1ilQKOFfMbGht2+A4VcEFnEBSY+AMoAshNwYgm09QjlMbuIA3JeexxNu3bVpAs6rBysW1bYFTQFIjYEnLCfXUb50iTLSxRRbR7GhmAyT1jZ06/k6YX8tx6iSpEbCZ5aMvdM5jiR2nNkiNgEsjaSs2rcf+N4tgmbHElxDGEjcnjCt2nDpJ6gQs6QeEzhzbETphdCDUY7OZhfwewgyWOxAG9gNsnX8rHSc/pHFGjsuAA4FZZtYROBKYlGXYx4G+hNkoV8RtZSGMdJx8kLocGFhnZosklUgqMbPnJV2VZdjvmlnvglrnOHkkjQJeIqk5MBH4m6QvCTlqNrwi6XtmNq1w5jlO/kijgPsSpsS5gPAdtyXhm265SJpG+ARVHxgq6WPC7ByZT1BdC2qx4+RI6gRsZsk667hyPW7K8YWwxXEKTeoEXKpDR0OgAbCyoo4cZU3k7jjFQOoEXLpDh6R+wP61ZI7jFJQ0fkbaBDN7jDA22HFSR+py4Mzg+0gJ0I2y+0g7TtGTOgETBt9nWE+YrL1v7ZjiOIUljQIea2ab9LySdDBFMLfVvLlz+c2vLmbRooVIJfQfMJBBp55WK7aMOLknQ0/sjiT++sgkbvj7C4w65ziOP6wrG81Y8L/lDPvdvcxdsLRW7HMCaVxedIqZ7VOZW75Zs776xfQFC75k4YIF7LZ7F1auXMFJA37IddffSOcdd8w5ztb7nVvlMLt33pa7rxzKIadezdp1G3jixnMYecUDfLloOctXhlVnzjn5MHbttC0jL78/J7tWv3WDLy+aB1KTA0s6COgOtCu1TtIWQL3asapqtGu3Fe3abQVAs2bN6dSpE19+Ob9aAs6FXTtuw+vTZrN6TRhd+dKbH9L38D25dtz/fe2naZNGpO3lX4ykRsCEb77NCdeU/JS0DOhfUcBET6wyqY2eWF988Tkz33uP73Xds6aTZsZHcxh9bh/atGzG6q/W0rtHF6a8G0Zjjh7Rh0HH78/SFavpPez6GrfN2ZQ0FqE7VLVjhqQOcXdE/L0n/g4CVplZmV0xJQ0DhgHccNOt+55x5rAcLP42q1au5PQhp/KTYcPpddTR1YorlyI0wGn9DuKsgYeycvVXvPfxPNasWcvFf/pmUs6fnX40jRvW5/e3PJVT/F6Ezg9pFPBzwIBSayPdb2bfzyLsJDM7uDK3sshHHRhg3bp1nHfOcLof3IPBQ6o/SWauAk4y5tw+fDF/Cbc9+NLXbttv25pHrj+bbgOuyClOF3B+SGNHjral10YCtsoybDNJPTIHkroDzfJsX7mYGaNH/YZOnTrlRbzVoV3r5gC036Y1fY/Yk/HPvEHn7dt9ff64w7oya/b82jLPiaSpDpxho6TtM1PoxOJxtrnjGcCdklrG4yVAjU0p+9aUN5nwxOPstPPODDwxfLo+7/wLOeTQwyoJmX/uu+YntGnVjHXrN3D+leNZsnw1N/9uEDt12IqNG43/zv1fzi3QTv5IYxG6N3Ab8GJ0OhQYZmZZzy4paQvCvcn6I2e+itD5Jh9F6ELgRej8kLoc2MyekbQPYVodAReY2cJsw0s6jjixu6RMnBWOJ3ac2iJ1Ao5sIPS8agzsLgkzm1hZIEm3AE2Bw4GxhM9PrxfSUMepDqlrxJL0E8J0Ov8ExsTf0VkG725mg4HFZjYGOAhoXwg7HScfpE7AwE+B/YBPzexwYG/CKoPZsCb+rpK0HWEwRMf8m+g4+SGNReg1ZrZGEpIamdlMSbtkGfYfkloBVwNTCK3XtxfMUsepJmkU8OdRhI8Bz0laDMzJMuxMYIOZPSxpd2CfGI/j1ElSJ2AzOyHujpb0PGFWymeyDP5bM3swduY4irDCw83AAfm31HGqTxrrwF9jZi+a2RNmtjbLIBvi73HALWb2OGGQhOPUSVIt4Bz4QtKtwEDgKUmN8Hvk1GH84dyUgYTPTr1jf+o2wM9r1yTHKZ/U1YGrg5mtAh5JHM8F5taeRY5TMZ4DO04R4wJ2nCLGBew4RUzqhhOmAUnDzOy22rajNHXVrs0Zz4HrJvmZXCv/1FW7NltcwI5TxLiAHaeIcQHXTepqPbOu2rXZ4o1YjlPEeA7sOEWMC9hxihgXcAGRtIOk6bVtR6HZXK6zLuICdpwixkcjFZ56km4nLH36BdAX+DGhU0RD4EPgVDNbJekuwsR6XYCtgQvNbIKkIcAJQCPCJHt/N7Mxki4DFprZnwEkXQ7MN7Oclg2U1AwYD3yXsCTrZcAuQB+gCfAKcJaZmaR9gTuBVcDLuaTn5AEz861AG7ADYWbLveLxeIJ4t0z4+T1wXty/izD9TwmwE/A5YW7rIYRhjVsShDQd6BbjnxLDlgAfJePOwd4fArcnjlsCbRLH9wB94v47wGFx/2pgem3f781x8yJ04fnEzN6O+28SRLeHpJfiusSDCDluhvFmttHMPgA+BnaN7s+Z2SIzW00Ys9zDzGYDiyTtDRwNvGVmi6ph6zSgl6SrJB1iYWmZwyW9Fm09AugS145qZWaZ5WvuKS9Cp7B4EbrwfJXY30DIQe8C+pnZ1Fg87pnwU/rDvFXiPpaQQ29DKNLmjJnNikXjY4E/SHqWsGZyNzP7TNJoQolAZdjj1AKeA9cOLYC5khoQcuAkAySVSOoMdALej+5HSWojqQnQD5gU3R8FehMms896AbeyiJPZrzKze4FrCNPqAiyU1Jyw1AwWphtamliKtfQ1ODWE58C1w2+B14BPCcXWFolz7xNWVtwaGG5xknpCQ9E9wI6ERqw3AMxsbZw+d4mZbaB6fA+4WtJGYB1wNuFlMQ2YDUxO+B1KWIp1FdV8cTi5410p6xCxFXqCmT1Uyn0IoRj7rbVCJZUQVpEYEOvNzmaEF6GLmLh6xIfAv1y8myeeAztOEeM5sOMUMS5gxyliXMCOU8S4gOswklbE3+0kPVSJ3/MlNa1i/D0lTaiC/xckdatKGk5hcQHXMJLqVTWMmc0xs/6VeDsfqJKAneLHBZwn4pjYmZLGSXpH0kOZHFHSbEmjJL1M6GnVWdIzkt6MfaJ3jf46SnpV0uQ40igZ9/S4X0/SNZKmxXTOkzQS2A54PnbqQNLRMa4pkh6MPamQ1Dva+TJwYjnX8q00yvBzs6Q3JM2QNCbhfqWkd2O4a6LbAEnTJU2VNDE/d9wBfDRSvjbCIAUDDo7HdwI/i/uzgYsTfv8F7BT3DwD+HfefAAbH/RHAikTc0+P+2cDDQP143CaRRtu43xaYCDSLx78ARhH6MX9GGOkkwuioCWVcS3lpvEDoUJJ0qxfduxJWc3yfbz5Ptoq/04DvJN18y8/mOXB++czMMn2U7wV6JM49ABBzwu7Ag5LeBm4Fto1+Dgbui/vljfDpRVh8fD2Amf2vDD8HArsDk2IapwEdCCObPjGzDyyo6d5qpDFQ0hTgLcJoqt2BZYTxzGMlnUgYKwyh3/Zdks4kCN7JE94XOr+UN2IIYGX8LSH0W94ryzhKk81IIBGGH568iaO0VxZhK01DUkfgZ8B+ZrY4dgFtbGbrJe0PHAmcBJwLHGFmwyUdABwHvC1pL6vesEcn4jlwftle0kFx/2TKmKnCzJYBn0gaAKDAnvH0JMKDD+WP8HkWGC6pfgzfJrov55tBEf8BDpa0Y/TTVNLOwEygYxzplLGxKmlk2ILwQloqaWvgmOivOdDSzJ4iNKrtFd07m9lrZjYKWAi0Lyddp4q4gPPLe8Bpkt4h1AdvLsffIOAMSVOBGYRpdgB+CoyQNJkwG0ZZjAX+C7wTw58S3W8Dnpb0vJktIIwRvi/a8h9gVzNbQ5jK58nYiPVpFdMAwMymEorOMwh1/Uy1oQUwIab5InBBdL86NohNJ9TNp5aTrlNFvC90npC0A6FBaI9aNsXZjPAc2HGKGM+BHaeI8RzYcYoYF7DjFDEuYMcpYlzAjlPEuIAdp4j5f79XL8BdV6cTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 288x144 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = metrics.confusion_matrix(y_valid, final_clf.predict(X_valid))\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "mpl.rc(\"figure\", figsize=(4, 2))\n",
    "\n",
    "hm = sns.heatmap(cm, \n",
    "            cbar=False,\n",
    "            annot=True, \n",
    "            square=True,\n",
    "            fmt='d',\n",
    "            yticklabels=['happy','sad'],\n",
    "            xticklabels=['happy','sad'],\n",
    "            cmap='Blues'\n",
    "            )\n",
    "plt.title('Confusion matrix - Validation dataset')\n",
    "plt.ylabel('actual class')\n",
    "plt.xlabel('predicted class')\n",
    "plt.tight_layout()\n",
    "#plt.savefig('./images/confmat_valid1.eps', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy, Precision, Recall, and F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom scorer methods to account for positive-negative class labels\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "# `pos_label` for positive class, since we have sad=1, happy=0\n",
    "\n",
    "acc_scorer = metrics.make_scorer(metrics.accuracy_score, greater_is_better=True)\n",
    "pre_scorer = metrics.make_scorer(metrics.precision_score, greater_is_better=True, pos_label=0)\n",
    "rec_scorer = metrics.make_scorer(metrics.recall_score, greater_is_better=True, pos_label=0)\n",
    "f1_scorer = metrics.make_scorer(metrics.f1_score, greater_is_better=True, pos_label=0)\n",
    "auc_scorer = metrics.make_scorer(metrics.roc_auc_score, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'Data':['Training', 'Validation'],\n",
    "     'ACC (%)':[],\n",
    "     'PRE (%)':[],\n",
    "     'REC (%)':[],\n",
    "     'F1 (%)':[],\n",
    "     'ROC AUC (%)':[],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['ACC (%)'].append(acc_scorer(estimator=final_clf, X=X_train, y_true=y_train))\n",
    "d['PRE (%)'].append(pre_scorer(estimator=final_clf, X=X_train, y_true=y_train))\n",
    "d['REC (%)'].append(rec_scorer(estimator=final_clf, X=X_train, y_true=y_train))\n",
    "d['F1 (%)'].append(f1_scorer(estimator=final_clf, X=X_train, y_true=y_train))\n",
    "d['ROC AUC (%)'].append(auc_scorer(estimator=final_clf, X=X_train, y_true=y_train))\n",
    "\n",
    "d['ACC (%)'].append(acc_scorer(estimator=final_clf, X=X_valid, y_true=y_valid))\n",
    "d['PRE (%)'].append(pre_scorer(estimator=final_clf, X=X_valid, y_true=y_valid))\n",
    "d['REC (%)'].append(rec_scorer(estimator=final_clf, X=X_valid, y_true=y_valid))\n",
    "d['F1 (%)'].append(f1_scorer(estimator=final_clf, X=X_valid, y_true=y_valid))\n",
    "d['ROC AUC (%)'].append(auc_scorer(estimator=final_clf, X=X_valid, y_true=y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACC (%)</th>\n",
       "      <th>PRE (%)</th>\n",
       "      <th>REC (%)</th>\n",
       "      <th>F1 (%)</th>\n",
       "      <th>ROC AUC (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Training</th>\n",
       "      <td>80.0</td>\n",
       "      <td>99.60</td>\n",
       "      <td>55.38</td>\n",
       "      <td>71.18</td>\n",
       "      <td>77.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation</th>\n",
       "      <td>54.5</td>\n",
       "      <td>88.89</td>\n",
       "      <td>15.24</td>\n",
       "      <td>26.02</td>\n",
       "      <td>56.57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ACC (%)  PRE (%)  REC (%)  F1 (%)  ROC AUC (%)\n",
       "Training       80.0    99.60    55.38   71.18        77.60\n",
       "Validation     54.5    88.89    15.24   26.02        56.57"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_perform = pd.DataFrame(d)\n",
    "df_perform = df_perform[['ACC (%)', 'PRE (%)', 'REC (%)', 'F1 (%)', 'ROC AUC (%)']]\n",
    "df_perform.index=(['Training', 'Validation'])\n",
    "df_perform = df_perform*100\n",
    "df_perform = np.round(df_perform, decimals=2)\n",
    "df_perform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_perform.to_csv('./clf_performance1x.csv', index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(document):\n",
    "\n",
    "    x_vect = vec.transform([document])\n",
    "    proba = np.max(final_clf.predict_proba(x_vect))\n",
    "    pred = final_clf.predict(x_vect)[0]\n",
    "    label = le.inverse_transform(pred)\n",
    "    return label, proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
